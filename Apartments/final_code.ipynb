{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "\n",
    "np.random.seed(2137) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>obj_type</th>\n",
       "      <th>dim_m2</th>\n",
       "      <th>n_rooms</th>\n",
       "      <th>floor_no</th>\n",
       "      <th>floor_max</th>\n",
       "      <th>year_built</th>\n",
       "      <th>dist_centre</th>\n",
       "      <th>n_poi</th>\n",
       "      <th>dist_sch</th>\n",
       "      <th>dist_clinic</th>\n",
       "      <th>dist_post</th>\n",
       "      <th>dist_kind</th>\n",
       "      <th>dist_rest</th>\n",
       "      <th>dist_uni</th>\n",
       "      <th>dist_pharma</th>\n",
       "      <th>own_type</th>\n",
       "      <th>build_mat</th>\n",
       "      <th>cond_class</th>\n",
       "      <th>has_park</th>\n",
       "      <th>has_balcony</th>\n",
       "      <th>has_lift</th>\n",
       "      <th>has_sec</th>\n",
       "      <th>has_store</th>\n",
       "      <th>price_z</th>\n",
       "      <th>src_month</th>\n",
       "      <th>loc_code</th>\n",
       "      <th>market_volatility</th>\n",
       "      <th>infrastructure_quality</th>\n",
       "      <th>neighborhood_crime_rate</th>\n",
       "      <th>popularity_index</th>\n",
       "      <th>green_space_ratio</th>\n",
       "      <th>estimated_maintenance_cost</th>\n",
       "      <th>global_economic_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24270</th>\n",
       "      <td>78a1de0708226437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.83</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>7.846</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.629</td>\n",
       "      <td>1.739</td>\n",
       "      <td>0.101</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>53cced8d</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>704287.28</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>693f303c</td>\n",
       "      <td>800711.71</td>\n",
       "      <td>32.67</td>\n",
       "      <td>48.48</td>\n",
       "      <td>62.38</td>\n",
       "      <td>0.999</td>\n",
       "      <td>9.04</td>\n",
       "      <td>105.668549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83142</th>\n",
       "      <td>115a285f1e8bbc20</td>\n",
       "      <td>0d6c4dfc</td>\n",
       "      <td>35.53</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>3.960</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.294</td>\n",
       "      <td>2.154</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.255</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7f8c00f9</td>\n",
       "      <td>53cced8d</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>313238.60</td>\n",
       "      <td>2023-08</td>\n",
       "      <td>8d5a4f0c</td>\n",
       "      <td>258100.50</td>\n",
       "      <td>37.87</td>\n",
       "      <td>26.23</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.88</td>\n",
       "      <td>104.927936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33582</th>\n",
       "      <td>a76803b46f21fb29</td>\n",
       "      <td>0d6c4dfc</td>\n",
       "      <td>25.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>6.563</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.090</td>\n",
       "      <td>1.316</td>\n",
       "      <td>0.061</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>493304.86</td>\n",
       "      <td>2024-04</td>\n",
       "      <td>e0cff11b</td>\n",
       "      <td>409311.56</td>\n",
       "      <td>75.92</td>\n",
       "      <td>17.89</td>\n",
       "      <td>50.33</td>\n",
       "      <td>0.999</td>\n",
       "      <td>12.23</td>\n",
       "      <td>90.167666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>6424c0db2a193b6b</td>\n",
       "      <td>0d6c4dfc</td>\n",
       "      <td>58.98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>4.714</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.317</td>\n",
       "      <td>2.250</td>\n",
       "      <td>1.087</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.152</td>\n",
       "      <td>2.146</td>\n",
       "      <td>0.569</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1991412.59</td>\n",
       "      <td>2023-10</td>\n",
       "      <td>693f303c</td>\n",
       "      <td>2008616.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.01</td>\n",
       "      <td>38.72</td>\n",
       "      <td>1.000</td>\n",
       "      <td>23.90</td>\n",
       "      <td>94.343251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9115</th>\n",
       "      <td>390664e65d2bd159</td>\n",
       "      <td>0c238f18</td>\n",
       "      <td>51.72</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.593</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.023</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>583340.31</td>\n",
       "      <td>2023-08</td>\n",
       "      <td>0ab06839</td>\n",
       "      <td>497593.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.23</td>\n",
       "      <td>35.79</td>\n",
       "      <td>1.000</td>\n",
       "      <td>9.70</td>\n",
       "      <td>104.708779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38195</th>\n",
       "      <td>467a68c5b1e2a638</td>\n",
       "      <td>0c238f18</td>\n",
       "      <td>44.99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>1.315</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.452</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>a2881958</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>696695.01</td>\n",
       "      <td>2023-11</td>\n",
       "      <td>693f303c</td>\n",
       "      <td>760572.57</td>\n",
       "      <td>78.62</td>\n",
       "      <td>54.11</td>\n",
       "      <td>69.87</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.46</td>\n",
       "      <td>95.629116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77428</th>\n",
       "      <td>223961fb32c9f378</td>\n",
       "      <td>2a6d5c01</td>\n",
       "      <td>55.91</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>0.251</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.249</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1372652.41</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>e0cff11b</td>\n",
       "      <td>1552494.92</td>\n",
       "      <td>1.30</td>\n",
       "      <td>53.47</td>\n",
       "      <td>69.91</td>\n",
       "      <td>1.000</td>\n",
       "      <td>7.97</td>\n",
       "      <td>107.830152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66138</th>\n",
       "      <td>d405785608a95062</td>\n",
       "      <td>0d6c4dfc</td>\n",
       "      <td>75.11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>16.980</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.686</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>a2881958</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>520878.21</td>\n",
       "      <td>2023-08</td>\n",
       "      <td>693f303c</td>\n",
       "      <td>522689.71</td>\n",
       "      <td>8.24</td>\n",
       "      <td>94.83</td>\n",
       "      <td>64.45</td>\n",
       "      <td>0.998</td>\n",
       "      <td>24.96</td>\n",
       "      <td>105.255044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17399</th>\n",
       "      <td>531952a298de2f42</td>\n",
       "      <td>0d6c4dfc</td>\n",
       "      <td>35.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1.664</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.468</td>\n",
       "      <td>1.633</td>\n",
       "      <td>0.381</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>373127.46</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>e0cff11b</td>\n",
       "      <td>381721.48</td>\n",
       "      <td>81.90</td>\n",
       "      <td>2.16</td>\n",
       "      <td>55.92</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.67</td>\n",
       "      <td>92.882208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81945</th>\n",
       "      <td>cd4b403eca3297f4</td>\n",
       "      <td>2a6d5c01</td>\n",
       "      <td>77.95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>4.882</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.266</td>\n",
       "      <td>2.365</td>\n",
       "      <td>0.062</td>\n",
       "      <td>12631efb</td>\n",
       "      <td>7ceffe3b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1689740.47</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>693f303c</td>\n",
       "      <td>1812499.92</td>\n",
       "      <td>2.06</td>\n",
       "      <td>20.78</td>\n",
       "      <td>44.19</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14.79</td>\n",
       "      <td>93.899531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                unit_id  obj_type  dim_m2  n_rooms  floor_no  floor_max  \\\n",
       "24270  78a1de0708226437       NaN   48.83      2.0       1.0        3.0   \n",
       "83142  115a285f1e8bbc20  0d6c4dfc   35.53      2.0       3.0        4.0   \n",
       "33582  a76803b46f21fb29  0d6c4dfc   25.87      2.0       3.0        4.0   \n",
       "1257   6424c0db2a193b6b  0d6c4dfc   58.98      3.0       NaN        3.0   \n",
       "9115   390664e65d2bd159  0c238f18   51.72      2.0       3.0        3.0   \n",
       "38195  467a68c5b1e2a638  0c238f18   44.99      2.0       3.0        6.0   \n",
       "77428  223961fb32c9f378  2a6d5c01   55.91      3.0       5.0        6.0   \n",
       "66138  d405785608a95062  0d6c4dfc   75.11      4.0       3.0        3.0   \n",
       "17399  531952a298de2f42  0d6c4dfc   35.35      1.0       NaN        3.0   \n",
       "81945  cd4b403eca3297f4  2a6d5c01   77.95      4.0      10.0       12.0   \n",
       "\n",
       "       year_built  dist_centre  n_poi  dist_sch  dist_clinic  dist_post  \\\n",
       "24270      1998.0        7.846   11.0     0.234        0.171      0.623   \n",
       "83142      1970.0        3.960    8.0     0.294        2.154      0.848   \n",
       "33582      1963.0        6.563   23.0     0.267        0.552      0.074   \n",
       "1257       2022.0        4.714   10.0     0.317        2.250      1.087   \n",
       "9115          NaN        0.593  121.0     0.016        0.108      0.258   \n",
       "38195      1961.0        1.315   81.0     0.133        0.535      0.173   \n",
       "77428      2023.0        0.251   98.0     0.495        0.728      0.312   \n",
       "66138      2011.0       16.980    2.0     0.758          NaN      0.720   \n",
       "17399      1965.0        1.664    9.0     0.278        0.783      0.223   \n",
       "81945      2020.0        4.882   17.0     0.355        0.112      0.135   \n",
       "\n",
       "       dist_kind  dist_rest  dist_uni  dist_pharma  own_type build_mat  \\\n",
       "24270      0.222      0.629     1.739        0.101  12631efb  7ceffe3b   \n",
       "83142      0.286      0.268     0.316        0.255  12631efb  7f8c00f9   \n",
       "33582      0.329      0.090     1.316        0.061  12631efb  7ceffe3b   \n",
       "1257       0.264      0.152     2.146        0.569  12631efb  7ceffe3b   \n",
       "9115       0.100      0.026     0.111        0.023  12631efb  7ceffe3b   \n",
       "38195      0.425      0.015     0.305        0.452  12631efb  7ceffe3b   \n",
       "77428      0.265      0.040     0.946        0.249  12631efb  7ceffe3b   \n",
       "66138      0.273      0.364       NaN        0.686  12631efb  7ceffe3b   \n",
       "17399      0.778      0.468     1.633        0.381  12631efb       NaN   \n",
       "81945      0.322      0.266     2.365        0.062  12631efb  7ceffe3b   \n",
       "\n",
       "      cond_class has_park has_balcony has_lift has_sec has_store     price_z  \\\n",
       "24270   53cced8d      yes         yes       no      no       yes   704287.28   \n",
       "83142   53cced8d       no          no       no      no       yes   313238.60   \n",
       "33582        NaN       no         yes       no      no       yes   493304.86   \n",
       "1257         NaN       no         yes      yes      no        no  1991412.59   \n",
       "9115         NaN       no          no       no      no        no   583340.31   \n",
       "38195   a2881958       no          no      yes      no        no   696695.01   \n",
       "77428        NaN       no          no      yes      no        no  1372652.41   \n",
       "66138   a2881958       no         yes       no     yes        no   520878.21   \n",
       "17399        NaN       no          no       no      no       yes   373127.46   \n",
       "81945        NaN       no          no      yes      no        no  1689740.47   \n",
       "\n",
       "      src_month  loc_code  market_volatility  infrastructure_quality  \\\n",
       "24270   2024-03  693f303c          800711.71                   32.67   \n",
       "83142   2023-08  8d5a4f0c          258100.50                   37.87   \n",
       "33582   2024-04  e0cff11b          409311.56                   75.92   \n",
       "1257    2023-10  693f303c         2008616.80                    1.00   \n",
       "9115    2023-08  0ab06839          497593.97                     NaN   \n",
       "38195   2023-11  693f303c          760572.57                   78.62   \n",
       "77428   2024-06  e0cff11b         1552494.92                    1.30   \n",
       "66138   2023-08  693f303c          522689.71                    8.24   \n",
       "17399   2024-01  e0cff11b          381721.48                   81.90   \n",
       "81945   2023-12  693f303c         1812499.92                    2.06   \n",
       "\n",
       "       neighborhood_crime_rate  popularity_index  green_space_ratio  \\\n",
       "24270                    48.48             62.38              0.999   \n",
       "83142                    26.23             48.00              1.000   \n",
       "33582                    17.89             50.33              0.999   \n",
       "1257                      3.01             38.72              1.000   \n",
       "9115                     92.23             35.79              1.000   \n",
       "38195                    54.11             69.87              1.000   \n",
       "77428                    53.47             69.91              1.000   \n",
       "66138                    94.83             64.45              0.998   \n",
       "17399                     2.16             55.92              1.000   \n",
       "81945                    20.78             44.19              1.000   \n",
       "\n",
       "       estimated_maintenance_cost  global_economic_index  \n",
       "24270                        9.04             105.668549  \n",
       "83142                        3.88             104.927936  \n",
       "33582                       12.23              90.167666  \n",
       "1257                        23.90              94.343251  \n",
       "9115                         9.70             104.708779  \n",
       "38195                        6.46              95.629116  \n",
       "77428                        7.97             107.830152  \n",
       "66138                       24.96             105.255044  \n",
       "17399                        6.67              92.882208  \n",
       "81945                       14.79              93.899531  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('appartments_train.csv')\n",
    "df_final_test = pd.read_csv('appartments_test.csv')\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['cond_class', 'build_mat', 'green_space_ratio', 'unit_id'])\n",
    "df_final_test = df_final_test.drop(columns=['cond_class', 'build_mat', 'green_space_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling NA for train set\n",
    "\n",
    "# setting \"obj_type\" to 'other\" if its missing\n",
    "df['obj_type'] = df['obj_type'].fillna('other')\n",
    "\n",
    "# using median value (4) for max floor if its missing\n",
    "df['floor_max'] = df['floor_max'].fillna(4)\n",
    "\n",
    "# if there is no floor, choose mid of the building\n",
    "df['floor_no'] = df['floor_no'].fillna(df['floor_max']/2)\n",
    "\n",
    "# for all distances I'll use the average of mean and median\n",
    "dist_columns = ['dist_centre', 'dist_sch','dist_clinic','dist_post', 'dist_kind',\n",
    "                'dist_rest', 'dist_uni', 'dist_pharma']\n",
    "\n",
    "for col in dist_columns:\n",
    "    avg_mean_median = (df[col].mean() + df[col].median()) / 2\n",
    "    df[col] = df[col].fillna(avg_mean_median)\n",
    "\n",
    "# managing all \"has..\" variables. If null happens - we will code it as \"no\". Then change into boolean\n",
    "has_columns = ['has_park', 'has_balcony', 'has_lift', 'has_sec', 'has_store']\n",
    "\n",
    "for col in has_columns:\n",
    "    df[col] = df[col].fillna('no')\n",
    "    df[col] = df[col].map({'no':0, 'yes':1})\n",
    "\n",
    "\n",
    "# dealing with other measures. If NA happen, I will use average of mean and median (to mitigate outliers). \n",
    "\n",
    "other_measures = ['market_volatility', 'infrastructure_quality', 'neighborhood_crime_rate', 'popularity_index',\n",
    "                'estimated_maintenance_cost', 'global_economic_index']\n",
    "\n",
    "for col in other_measures:\n",
    "    avg_mean_median = (df[col].mean() + df[col].median()) / 2\n",
    "    df[col] = df[col].fillna(avg_mean_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling NA for test set\n",
    "\n",
    "# setting \"obj_type\" to 'other\" if its missing\n",
    "df_final_test['obj_type'] = df_final_test['obj_type'].fillna('other')\n",
    "\n",
    "# using median value (4) for max floor if its missing\n",
    "df_final_test['floor_max'] = df_final_test['floor_max'].fillna(4)\n",
    "\n",
    "# if there is no floor, choose mid of the building\n",
    "df_final_test['floor_no'] = df_final_test['floor_no'].fillna(df_final_test['floor_max']/2)\n",
    "\n",
    "# for all distances I'll use the average of mean and median\n",
    "dist_columns = ['dist_centre', 'dist_sch','dist_clinic','dist_post', 'dist_kind',\n",
    "                'dist_rest', 'dist_uni', 'dist_pharma']\n",
    "\n",
    "for col in dist_columns:\n",
    "    avg_mean_median = (df_final_test[col].mean() + df_final_test[col].median()) / 2\n",
    "    df_final_test[col] = df_final_test[col].fillna(avg_mean_median)\n",
    "\n",
    "# managing all \"has..\" variables. If null happens - we will code it as \"no\". Then change into boolean\n",
    "has_columns = ['has_park', 'has_balcony', 'has_lift', 'has_sec', 'has_store']\n",
    "\n",
    "for col in has_columns:\n",
    "    df_final_test[col] = df_final_test[col].fillna('no')\n",
    "    df_final_test[col] = df_final_test[col].map({'no':0, 'yes':1})\n",
    "\n",
    "\n",
    "# dealing with other measures. If NA happen, I will use average of mean and median (to mitigate outliers). \n",
    "\n",
    "other_measures = ['market_volatility', 'infrastructure_quality', 'neighborhood_crime_rate', 'popularity_index',\n",
    "                'estimated_maintenance_cost', 'global_economic_index']\n",
    "\n",
    "for col in other_measures:\n",
    "    avg_mean_median = (df_final_test[col].mean() + df_final_test[col].median()) / 2\n",
    "    df_final_test[col] = df_final_test[col].fillna(avg_mean_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Step 1: Identify indices\n",
    "known_mask = df['year_built'].notna()\n",
    "missing_mask = df['year_built'].isna()\n",
    "\n",
    "# Step 2: Define features\n",
    "features = ['floor_no', 'floor_max', 'n_rooms', 'dim_m2', 'dist_centre', 'obj_type',\n",
    "            'price_z', 'infrastructure_quality', 'has_lift', 'estimated_maintenance_cost']\n",
    "\n",
    "# Step 3: Prepare known data\n",
    "df_known = df.loc[known_mask, features + ['year_built']].copy()\n",
    "X_known = pd.get_dummies(df_known[features])\n",
    "y_known = df_known['year_built']\n",
    "\n",
    "# Step 4: Prepare missing data (use same dummies structure!)\n",
    "df_missing = df.loc[missing_mask, features].copy()\n",
    "X_missing = pd.get_dummies(df_missing)\n",
    "\n",
    "# Step 5: Align columns in case of missing dummy columns in either set\n",
    "X_missing = X_missing.reindex(columns=X_known.columns, fill_value=0)\n",
    "\n",
    "# Step 6: Train model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_known, y_known)\n",
    "\n",
    "# Step 7: Predict and impute\n",
    "df.loc[missing_mask, 'year_built'] = rf.predict(X_missing).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Step 1: Identify indices\n",
    "known_mask = df_final_test['year_built'].notna()\n",
    "missing_mask = df_final_test['year_built'].isna()\n",
    "\n",
    "# Step 2: Define features\n",
    "features = ['floor_no', 'floor_max', 'n_rooms', 'dim_m2', 'dist_centre', 'obj_type',\n",
    "             'infrastructure_quality', 'has_lift', 'estimated_maintenance_cost']\n",
    "\n",
    "# Step 3: Prepare known data\n",
    "df_known = df_final_test.loc[known_mask, features + ['year_built']].copy()\n",
    "X_known = pd.get_dummies(df_known[features])\n",
    "y_known = df_known['year_built']\n",
    "\n",
    "# Step 4: Prepare missing data (use same dummies structure!)\n",
    "df_missing = df_final_test.loc[missing_mask, features].copy()\n",
    "X_missing = pd.get_dummies(df_missing)\n",
    "\n",
    "# Step 5: Align columns in case of missing dummy columns in either set\n",
    "X_missing = X_missing.reindex(columns=X_known.columns, fill_value=0)\n",
    "\n",
    "# Step 6: Train model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_known, y_known)\n",
    "\n",
    "# Step 7: Predict and impute\n",
    "df_final_test.loc[missing_mask, 'year_built'] = rf.predict(X_missing).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['src_month'] = pd.to_datetime(df['src_month'], format='%Y-%m')\n",
    "df['src_year'] = df['src_month'].dt.year\n",
    "df['src_month'] = df['src_month'].dt.month\n",
    "\n",
    "df_final_test['src_month'] = pd.to_datetime(df_final_test['src_month'], format='%Y-%m')\n",
    "df_final_test['src_year'] = df_final_test['src_month'].dt.year\n",
    "df_final_test['src_month'] = df_final_test['src_month'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if apartment is on the last floor\n",
    "df['last_floor'] = np.where(df['floor_no'] == df['floor_max'], 1, 0)\n",
    "\n",
    "df_final_test['last_floor'] = np.where(df_final_test['floor_no'] == df_final_test['floor_max'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['room_size'] = df['dim_m2'] / df['n_rooms']\n",
    "\n",
    "df['apart_age'] = df['src_year'] - df['year_built']\n",
    "\n",
    "df['is_old_building'] = np.where((df['apart_age'] >= 100), 1, 0)\n",
    "df['is_new_building'] = np.where((df['apart_age'] <= 10), 1, 0)\n",
    "\n",
    "df['has_all_amenities'] = np.where((df['has_park'] == 1) & (df['has_balcony'] == 1) & (df['has_lift'] == 1) & (df['has_sec'] == 1), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "df_final_test['room_size'] = df_final_test['dim_m2'] / df_final_test['n_rooms']\n",
    "\n",
    "df_final_test['apart_age'] = df_final_test['src_year'] - df_final_test['year_built']\n",
    "\n",
    "df_final_test['is_old_building'] = np.where((df_final_test['apart_age'] >= 100), 1, 0)\n",
    "df_final_test['is_new_building'] = np.where((df_final_test['apart_age'] <= 10), 1, 0)\n",
    "\n",
    "df_final_test['has_all_amenities'] = np.where((df_final_test['has_park'] == 1) & (df_final_test['has_balcony'] == 1) & (df_final_test['has_lift'] == 1) & (df_final_test['has_sec'] == 1), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "vars_to_transform = [\n",
    "    'dist_centre', 'dist_uni', 'infrastructure_quality',\n",
    "    'estimated_maintenance_cost', 'dist_clinic', 'dist_post', 'dist_sch',\n",
    "    'dist_pharma', 'dist_kind', 'dist_rest', 'n_poi', 'room_size', 'apart_age'\n",
    "]\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "df[vars_to_transform] = pt.fit_transform(df[vars_to_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "vars_to_transform = [\n",
    "    'dist_centre', 'dist_uni', 'infrastructure_quality',\n",
    "    'estimated_maintenance_cost', 'dist_clinic', 'dist_post', 'dist_sch',\n",
    "    'dist_pharma', 'dist_kind', 'dist_rest', 'n_poi', 'room_size', 'apart_age'\n",
    "]\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "df_final_test[vars_to_transform] = pt.fit_transform(df_final_test[vars_to_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA component loadings (positive → further, negative → closer):\n",
      "n_poi         -0.438512\n",
      "dist_kind      0.241474\n",
      "dist_centre    0.296308\n",
      "dist_uni       0.317427\n",
      "dist_post      0.318704\n",
      "dist_clinic    0.321495\n",
      "dist_sch       0.341227\n",
      "dist_pharma    0.343904\n",
      "dist_rest      0.348141\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define distance-related variables\n",
    "distance_vars = [\n",
    "    'dist_centre', 'dist_sch', 'dist_clinic', 'dist_post', 'dist_kind',\n",
    "    'dist_rest', 'dist_uni', 'dist_pharma', 'n_poi'\n",
    "]\n",
    "\n",
    "# Extract data for PCA\n",
    "X_dist = df[distance_vars]\n",
    "\n",
    "# Standardize the distance variables\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_dist)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=1)\n",
    "accessibility_scores = pca.fit_transform(X_scaled).ravel()\n",
    "\n",
    "# Assign meta-variable back to the original DataFrame\n",
    "df['overall_accessibility'] = accessibility_scores\n",
    "\n",
    "# Show contribution of each feature to the PCA component\n",
    "print(\"PCA component loadings (positive → further, negative → closer):\")\n",
    "print(pd.Series(pca.components_[0], index=distance_vars).sort_values())\n",
    "\n",
    "df = df.drop(distance_vars, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA component loadings (positive → further, negative → closer):\n",
      "n_poi         -0.438349\n",
      "dist_kind      0.245141\n",
      "dist_centre    0.298229\n",
      "dist_uni       0.317810\n",
      "dist_post      0.318582\n",
      "dist_clinic    0.321286\n",
      "dist_sch       0.337442\n",
      "dist_pharma    0.343294\n",
      "dist_rest      0.348391\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define distance-related variables\n",
    "distance_vars = [\n",
    "    'dist_centre', 'dist_sch', 'dist_clinic', 'dist_post', 'dist_kind',\n",
    "    'dist_rest', 'dist_uni', 'dist_pharma', 'n_poi'\n",
    "]\n",
    "\n",
    "# Extract data for PCA\n",
    "X_dist = df_final_test[distance_vars]\n",
    "\n",
    "# Standardize the distance variables\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_dist)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=1)\n",
    "accessibility_scores = pca.fit_transform(X_scaled).ravel()\n",
    "\n",
    "# Assign meta-variable back to the original DataFrame\n",
    "df_final_test['overall_accessibility'] = accessibility_scores\n",
    "\n",
    "# Show contribution of each feature to the PCA component\n",
    "print(\"PCA component loadings (positive → further, negative → closer):\")\n",
    "print(pd.Series(pca.components_[0], index=distance_vars).sort_values())\n",
    "\n",
    "df_final_test = df_final_test.drop(distance_vars, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_price'] = np.log(df['price_z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turing year into categorical and one hot encoding. Dropping first to omit collinearity\n",
    "\n",
    "bins = [0, 1900, 1920, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 2030]\n",
    "labels = [\n",
    "    'before_1900', '1900_1920', '1920_1940', '1940_1950', '1950_1960', '1960_1970',\n",
    "    '1970_1980', '1980_1990', '1990_2000', '2000_2010', '2010_2020', 'after_2020'\n",
    "]\n",
    "\n",
    "df['year_built_cat'] = pd.cut(df['year_built'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['year_built_cat'], prefix='', prefix_sep='', drop_first=True)\n",
    "\n",
    "\n",
    "df = df.drop(columns=['year_built'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turing year into categorical and one hot encoding. Dropping first to omit collinearity\n",
    "\n",
    "bins = [0, 1900, 1920, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 2030]\n",
    "labels = [\n",
    "    'before_1900', '1900_1920', '1920_1940', '1940_1950', '1950_1960', '1960_1970',\n",
    "    '1970_1980', '1980_1990', '1990_2000', '2000_2010', '2010_2020', 'after_2020'\n",
    "]\n",
    "\n",
    "df_final_test['year_built_cat'] = pd.cut(df_final_test['year_built'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "df_final_test = pd.get_dummies(df_final_test, columns=['year_built_cat'], prefix='', prefix_sep='', drop_first=True)\n",
    "\n",
    "\n",
    "df_final_test = df_final_test.drop(columns=['year_built'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turing obj_type into dummies - only 4 options\n",
    "df = pd.get_dummies(df, columns=['obj_type'], prefix='obj_type', drop_first=True)\n",
    "\n",
    "# similar for own_type\n",
    "df = pd.get_dummies(df, columns=['own_type'], prefix='own_type', drop_first=True)\n",
    "\n",
    "# and finally for loc_code\n",
    "df = pd.get_dummies(df, columns=['loc_code'], prefix='loc_code', drop_first=True)\n",
    "\n",
    "# and for src_year, why not\n",
    "df = pd.get_dummies(df, columns=['src_year'], prefix='src_year', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turing obj_type into dummies - only 4 options\n",
    "df_final_test = pd.get_dummies(df_final_test, columns=['obj_type'], prefix='obj_type', drop_first=True)\n",
    "\n",
    "# similar for own_type\n",
    "df_final_test = pd.get_dummies(df_final_test, columns=['own_type'], prefix='own_type', drop_first=True)\n",
    "\n",
    "# and finally for loc_code\n",
    "df_final_test = pd.get_dummies(df_final_test, columns=['loc_code'], prefix='loc_code', drop_first=True)\n",
    "\n",
    "# and for src_year, why not\n",
    "df_final_test = pd.get_dummies(df_final_test, columns=['src_year'], prefix='src_year', drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: (109517, 54)\n",
      "Validation: (23468, 54)\n",
      "Test: (23469, 54)\n",
      "\n",
      "Missing value counts:\n",
      "Training set: 0 missing values\n",
      "Validation set: 0 missing values\n",
      "Test set: 0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target variable\n",
    "X = df.drop(columns=['price_z', 'log_price'])  # All features except targetget\n",
    "y = df['log_price']  # Target variable\n",
    "\n",
    "# Initial split: 70% training, 30% temporary holdout (stratified)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.30, \n",
    "    random_state=420  # Reproducibility\n",
    ")\n",
    "\n",
    "# Split temporary holdout into validation and test sets (50/50 of the 30%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=0.50, \n",
    "    random_state=69  # Reproducibility\n",
    ")\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Check for missing values in each dataset\n",
    "print(\"\\nMissing value counts:\")\n",
    "print(f\"Training set: {X_train.isna().sum().sum()} missing values\")\n",
    "print(f\"Validation set: {X_val.isna().sum().sum()} missing values\")\n",
    "print(f\"Test set: {X_test.isna().sum().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "\n",
    "to_scale = [\n",
    "    'dim_m2', 'n_rooms', 'floor_no', 'floor_max', 'src_month',\n",
    "    'market_volatility', 'infrastructure_quality', 'neighborhood_crime_rate',\n",
    "    'popularity_index', 'estimated_maintenance_cost', 'global_economic_index',\n",
    "    'overall_accessibility', 'room_size', 'apart_age'\n",
    "]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "X_train[to_scale] = scaler.fit_transform(X_train[to_scale])\n",
    "\n",
    "# Step 2: Transform validation and test data with the same scaler\n",
    "X_val[to_scale] = scaler.transform(X_val[to_scale])\n",
    "X_test[to_scale] = scaler.transform(X_test[to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "def train_and_tune_regressor(model, param_grid, X_train, y_train, X_val, y_val, \n",
    "                             model_name='Model', cv=5, n_jobs=-1, verbose=1):\n",
    "    \"\"\"\n",
    "    Trains and tunes a regression model using GridSearchCV.\n",
    "    \n",
    "    Args:\n",
    "        model: Base regressor model\n",
    "        param_grid: Dictionary of hyperparameters to tune\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_val: Validation features\n",
    "        y_val: Validation target\n",
    "        model_name: Name for model identification\n",
    "        cv: Number of cross-validation folds\n",
    "        n_jobs: Number of jobs to run in parallel\n",
    "        verbose: Controls verbosity\n",
    "        \n",
    "    Returns:\n",
    "        best_estimator: The best performing model from GridSearchCV\n",
    "        results: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create scoring dictionary\n",
    "    scoring = {\n",
    "        'RMSE': make_scorer(lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)), greater_is_better=False),\n",
    "        'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "        'R2': 'r2'\n",
    "    }\n",
    "\n",
    "    # Create pipeline with only the regressor\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    # Configure GridSearchCV\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid={'regressor__' + k: v for k, v in param_grid.items()},  # Add regressor prefix\n",
    "        scoring=scoring,\n",
    "        refit='RMSE',  # Metric to choose best model\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Train model with hyperparameter tuning\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = grid.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "        'R2': r2_score(y_val, y_val_pred)\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "    print(f\"Validation RMSE: {val_metrics['RMSE']:.4f}\")\n",
    "    print(f\"Validation MAE: {val_metrics['MAE']:.4f}\")\n",
    "    print(f\"Validation R2: {val_metrics['R2']:.4f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return grid.best_estimator_, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Ridge Regression Results\n",
      "==================================================\n",
      "Best parameters: {'regressor__alpha': 1e-05, 'regressor__fit_intercept': True, 'regressor__positive': False, 'regressor__solver': 'auto'}\n",
      "Validation RMSE: 0.1585\n",
      "Validation MAE: 0.1217\n",
      "Validation R2: 0.9002\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge()\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [1e-05],\n",
    "    'fit_intercept': [True],\n",
    "    'solver': ['auto'],\n",
    "    'positive': [False]\n",
    "}\n",
    "\n",
    "best_ridge, metrics_ridge = train_and_tune_regressor(\n",
    "    model=ridge_model,\n",
    "    param_grid=param_grid_ridge,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_test,\n",
    "    y_val=y_test,\n",
    "    model_name='Ridge Regression'\n",
    ")\n",
    "\n",
    "final_model = best_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORECASTING ON FINAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                unit_id          price\n",
      "0      553836e34a0d5d6b  525417.367695\n",
      "1      e04d0521349583d9  559229.021683\n",
      "2      83179d073b8319db  629865.034573\n",
      "3      9af01b0ba6016d69  338683.195217\n",
      "4      086cba055ca54004  367337.444544\n",
      "...                 ...            ...\n",
      "39109  9be7614c59aaf47a  410898.018954\n",
      "39110  1773aa7b6d9e1d0d  512037.779918\n",
      "39111  cc792c54b40cef6a  398464.682503\n",
      "39112  7c84f583161c9626  318576.389531\n",
      "39113  45bfa939d28ac6d4  451927.620388\n",
      "\n",
      "[39114 rows x 2 columns]\n",
      "✅ Predictions saved to final_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Save case_id\n",
    "case_ids = df_final_test['unit_id']\n",
    "\n",
    "# 2. Drop case_id before prediction\n",
    "X_final = df_final_test.drop(columns=['unit_id'])\n",
    "\n",
    "# 3. Ensure scaling/transformations match training set (if you used one)\n",
    "X_final[to_scale] = scaler.transform(X_final[to_scale])\n",
    "\n",
    "# 4. Predict (assumes model trained on log_price)\n",
    "y_pred_log = final_model.predict(X_final)\n",
    "\n",
    "# 5. Convert back from log to real price\n",
    "y_pred_price = np.exp(y_pred_log)\n",
    "\n",
    "# 6. Combine into final output\n",
    "final_submission = pd.DataFrame({\n",
    "    'unit_id': case_ids,\n",
    "    'price': y_pred_price\n",
    "})\n",
    "\n",
    "print(final_submission)\n",
    "# 7. Save to CSV\n",
    "final_submission.to_csv('final_predictions_regression.csv', index=False)\n",
    "\n",
    "# Done!\n",
    "print(\"✅ Predictions saved to final_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_workshop",
   "language": "python",
   "name": "nlp_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
